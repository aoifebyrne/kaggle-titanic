{
  "metadata": {
    "kernelspec": {
      "name": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.5.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "## Introduction ##\n\nThis notebook is written in Python. \n\nSteps:\n\n 1. Explore and visualize the data.\n 2. Feature engineering and imputing missing data\n 3. Compare the accuracy of classifiers\n 4. Predict survival using an ensemble of classifiers\n\n###Question and problem definition###\n\n\"  On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. \"\n\n\"In this challenge, we ask you to complete the analysis of what sorts of people were likely to survive. In particular, we ask you to apply the tools of machine learning to predict which passengers survived the tragedy.\"",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "#Import libraries and data\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport re as re\nimport matplotlib.pyplot as plt\n\n%matplotlib inline\n\n\ntrain = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')\n\nfull_data = [train, test]",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "print(train.head())\ntrain.describe()\ntrain.info()",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "from numpy import corrcoef\n\ncorrcoef(train[\"PassengerId\"], train[\"Survived\"])",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "print (train[[\"Sex\", \"Survived\"]].groupby(['Sex'], as_index=False).mean())",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "print (train[[\"Parch\", \"Survived\"]].groupby(['Parch'], as_index=False).mean())",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "train[\"Famsize\"] = train[\"Parch\"]+ train[\"SibSp\"]+1\ntest[\"Famsize\"] = test[\"Parch\"]+ test[\"SibSp\"]+1\n\n\nprint (train[[\"Famsize\", \"Survived\"]].groupby(['Famsize'], as_index=False).count())",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "#No parents\n\nprint(train.loc[(train[\"Parch\"]==0) & (train[\"Age\"]<18)])\n\ntrain[\"LoneChild\"] = 0\ntrain.loc[(train[\"Parch\"]==0) & (train[\"Age\"]<18), \"LoneChild\"] = 1",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "print (train[[\"LoneChild\", \"Survived\"]].groupby(['LoneChild'], as_index=False).mean())\nsns.factorplot(x=\"LoneChild\", y=\"Survived\", data=train)",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "train = train.drop(\"LoneChild\", 1)",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "for dataset in [train, test]:\n    dataset['Alone'] = 0\n    dataset.loc[dataset['Famsize'] == 1, 'Alone'] = 1\n    \nsns.factorplot(x=\"Alone\", y=\"Survived\", data=train)",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "sns.factorplot('Embarked','Survived', data=train)",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "print(\"Mean\")\nprint (train[[\"Embarked\", \"Survived\"]].groupby(['Embarked'], as_index=False).mean())\n\nprint(\"Count\")\nprint (train[[\"Embarked\", \"Survived\"]].groupby(['Embarked'], as_index=False).count())",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "sns.countplot(x='Survived', hue=\"Embarked\", data=train, order=[1,0])",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "train[\"Embarked\"] = train[\"Embarked\"].fillna('S')\ntest[\"Embarked\"] = test[\"Embarked\"].fillna('S')",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "for dataset in full_data:\n    dataset['Fare'] = dataset['Fare'].fillna(train['Fare'].median())",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "for dataset in [train, test]:\n    avg_age = dataset['Age'].mean()\n    std_age = dataset['Age'].std()\n    age_null_count = dataset['Age'].isnull().sum()\n    \n    random_age = np.random.randint(avg_age - std_age , avg_age + std_age , size=age_null_count)\n    dataset['Age'][dataset['Age'].isnull()] = age_null_count\n    dataset['Age'] = dataset['Age'].astype(int)\n    ",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "np.corrcoef(train[\"Age\"], train[\"Survived\"])",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# peaks for survived/not survived passengers by their age\nfacet = sns.FacetGrid(train, hue=\"Survived\",aspect=3)\nfacet.map(sns.kdeplot,'Age',shade= True)\nfacet.set(xlim=(0, train['Age'].max()))\nfacet.add_legend()",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "for dataset in [train, test]:\n    dataset['Title'] = dataset.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n\nprint(train['Title'].value_counts())",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "#cleaning up the title column\n\nfor data_set in [train, test]:\n    data_set['Title'] = data_set['Title'].replace('Mlle', 'Ms')\n    data_set['Title'] = data_set['Title'].replace('Miss', 'Ms')\n    data_set['Title'] = data_set['Title'].replace('Mme', 'Mrs')\n    data_set['Title'] = data_set['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "from sklearn.preprocessing import LabelEncoder\n\ncategories = ['Embarked','Sex','Title']\n\nfor cat in categories:\n    train[cat] = LabelEncoder().fit_transform(train[cat])\n    test[cat] = LabelEncoder().fit_transform(test[cat])",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "drop_elements = ['Name', 'Ticket', 'Cabin', 'SibSp',\\\n                 'Parch']\n\ntrain = train.drop(drop_elements, axis = 1)\ntest = test.drop(drop_elements, axis = 1)",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "#check everything looks good\n\ntrain.info()",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "###Comparing algorithms#",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "from sklearn.cross_validation import train_test_split\nfrom sklearn.cross_validation import cross_val_score\n\nfrom sklearn.model_selection import StratifiedShuffleSplit\nfrom sklearn.metrics import accuracy_score, log_loss\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\nfrom sklearn.linear_model import LogisticRegression\n\nclassifiers = [\n    [KNeighborsClassifier(3),'KNN'],\n    [SVC(probability=True), 'SVC'],\n    [DecisionTreeClassifier(),'Decision Tree'],\n    [RandomForestClassifier(),'Random Forest'],\n    [AdaBoostClassifier(),'ADA booster'],\n    [GradientBoostingClassifier(),'Gradient Booster'],\n    [GaussianNB(),'Gaussian Nb'],\n    [LinearDiscriminantAnalysis(),'Linear Discriminant Analysis'],\n    [QuadraticDiscriminantAnalysis(),'Quadratic Discrimination'],\n    [LogisticRegression(),'Logistic Regression']]\n\n\nX = train.drop(\"Survived\",axis=1)\ny = train[\"Survived\"]\nX_test  = test\n\n\n\nscores = []\n\nfor clf in classifiers:\n    \n    clf = clf[0]\n    \n    clf.fit(X,y)\n    y_pred = clf.predict(X_test)\n    \n    cv_scores = cross_val_score(clf, X, y, cv=5)\n\n    #score = clf.score(X,y)\n    scores.append(cv_scores.mean())\n    ",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "#viewing classifier scores\n\nnames = [clf[1] for clf in classifiers]\n\n\nnp.column_stack((names, scores))",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\n\ntest[\"PassengerId\"] = test[\"PassengerId\"].astype(int)\n\npredictors = [\"Pclass\", \"Sex\", \"Age\", \"Fare\", \"Embarked\", \"Famsize\", \"Title\", \"Alone\"]\n\nalgorithms = [GaussianNB(), LinearDiscriminantAnalysis(), GradientBoostingClassifier(random_state=1, n_estimators=25, max_depth=3),\nLogisticRegression(random_state=1),RandomForestClassifier(random_state=1, n_estimators = 50, min_samples_split=4, min_samples_leaf=2)]\n\n\npredictions = []\ntrain_target = train[\"Survived\"]\nfull_test_predictions = []\n\n    # Make predictions for each algorithm on each fold\nfor alg in algorithms:\n        # Fit the algorithm on the training data\n    alg.fit(train[predictors], train_target)\n        # Select and predict on the test fold \n        # We need to use .astype(float) to convert the dataframe to all floats and avoid an sklearn error\n    test_predictions = alg.predict_proba(test[predictors])[:,1]\n    full_test_predictions.append(test_predictions)\n    # Use a simple ensembling scheme&#8212;just average the predictions to get the final classification\ntest_predictions = ( sum(full_test_predictions) / len(full_test_predictions) )\n    # Any value over .5 is assumed to be a 1 prediction, and below .5 is a 0 prediction\ntest_predictions[test_predictions <= .5] = 0\ntest_predictions[test_predictions > .5] = 1\npredictions.append(test_predictions)\n\n# Put all the predictions together into one array\npredictions = np.concatenate(predictions, axis=0).astype(int)\n\n\n\nsubmission = pd.DataFrame({\n        \"PassengerId\": test[\"PassengerId\"],\n        \"Survived\": predictions\n    })\n\nsubmission.to_csv('titanic-predictions-4.csv', index = False)",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": null,
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    }
  ]
}